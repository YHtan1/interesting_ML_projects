{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "# read files\n",
    "import os\n",
    "# import torch and related libraries for machine learning\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.io import read_image\n",
    "import pandas as pd # for data manipulation\n",
    "from tqdm import tqdm # for iteration \n",
    "import matplotlib.pyplot as plt # visualization \n",
    "import optuna # type: ignore # optimization \n",
    "from sklearn.metrics import accuracy_score\n",
    "from IPython.core.debugger import set_trace # for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed in torch\n",
    "torch.manual_seed(235)\n",
    "\n",
    "# If using CUDA, set seed to CUDA\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(235)\n",
    "    torch.cuda.manual_seed_all(235)  # for multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom Dataset class - for  better accessibility\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "    \n",
    "    def __len__self(self, idx):\n",
    "        return len(self.img_labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = read_image(img_path)\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "# read data\n",
    "training_data = MNIST(root=\"./data\", train=True, download=True, transform=transforms.ToTensor())\n",
    "test_data = MNIST(root=\"./data\", train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "# use 20% of training data for validation\n",
    "train_set_size = int(len(training_data) * 0.8)\n",
    "valid_set_size = len(training_data) - train_set_size\n",
    "\n",
    "# split the train set into two\n",
    "seed = torch.Generator().manual_seed(42)\n",
    "train_set, valid_set = torch.utils.data.random_split(training_data, [train_set_size, valid_set_size], generator=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate Dataloader\n",
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True, num_workers=1)\n",
    "valid_dataloader = DataLoader(valid_set, batch_size=64, shuffle=True, num_workers=1)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: torch.Size([28, 28]), Label: 2\n",
      "Image shape: torch.Size([28, 28]), Label: 7\n",
      "Image shape: torch.Size([28, 28]), Label: 6\n",
      "Image shape: torch.Size([28, 28]), Label: 1\n",
      "Image shape: torch.Size([28, 28]), Label: 7\n",
      "Epoch 0:   7%|▋         | 70/938 [22:12<4:35:29,  0.05it/s, v_num=26, train_loss_step=1.750, train_acc_step=-0.754]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# View the data\n",
    "plt.figure(figsize=(15, 5))  # Adjust figure size for better visibility\n",
    "\n",
    "# Iterate through the dataset and print out images, dimensions, and labels\n",
    "for i, (image, label) in enumerate(train_dataloader):\n",
    "    if i < 5:\n",
    "        print(f\"Image shape: {image[0][0].shape}, Label: {label[0].item()}\")  # Print image shape and label\n",
    "        \n",
    "        plt.subplot(1, 5, i + 1)  # Create a subplot\n",
    "        plt.imshow(image[0].squeeze(), cmap='gray')  # Display the image\n",
    "        plt.title(f\"Label: {label[0].item()}\", fontsize=12)  # Add the label as the title\n",
    "        plt.axis('off')  # Hide axes for better visualization\n",
    "    else:\n",
    "        break\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"mlp_trials\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, weight_init=nn.init.xavier_uniform_, \n",
    "               loss_fn=nn.CrossEntropyLoss(), opt=torch.optim.Adam, lr=0.0001,\n",
    "               weight_decay = 0.0, d = None, dropout_prob=None, \n",
    "               **kwargs\n",
    "               ):\n",
    "        # inherit from nn.Module (base class for all Modules)\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.weight_init = weight_init\n",
    "        self.loss_fn = loss_fn\n",
    "        self.loss_list = list()\n",
    "        self.dropout_prob = dropout_prob\n",
    "\n",
    "        # initiate layers\n",
    "        self.layers = nn.Sequential(\n",
    "        nn.Flatten(),\n",
    "        self.seq_block(self.input_dim, 64), \n",
    "        self.seq_block(64, 128), \n",
    "        self.seq_block(128, 256),\n",
    "        self.seq_block(256, 128),\n",
    "        self.seq_block(128, 64),\n",
    "        nn.Linear(64, output_dim)\n",
    "        )\n",
    "\n",
    "        # initialize weights \n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "        # optimizer parameters\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.opt = opt(self.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
    "\n",
    "    # sequential block \n",
    "    def seq_block(self, input_dim_, output_dim_):\n",
    "        seq_layers = nn.Sequential(\n",
    "            nn.Linear(input_dim_, output_dim_),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(output_dim_)\n",
    "        )\n",
    "        if self.dropout_prob:\n",
    "            seq_layers.append(nn.Dropout(self.dropout_prob))\n",
    "        return seq_layers\n",
    "    \n",
    "    # function to initialize weights\n",
    "    def init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            self.weight_init(module.weight)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use lighting for the training loop \n",
    "import lightning as L\n",
    "\n",
    "class MLPWrapper(L.LightningModule):\n",
    "    def __init__(self, model, lr=0.0001, weight_decay=0.0):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, target = batch\n",
    "        predictions = self.model(inputs)\n",
    "        loss = self.loss_fn(predictions, target)\n",
    "        acc = (predictions.argmax(dim=1)==target).float().mean()\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"train_acc\", acc, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        inputs, target = batch\n",
    "        predictions = self.model(inputs)\n",
    "        loss = self.loss_fn(predictions, target)\n",
    "        acc= (predictions.argmax(dim=1)==target).float().mean()\n",
    "        self.log(\"val_loss\", loss, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"val_acc\", acc, on_epoch=True, prog_bar=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        inputs, target = batch\n",
    "        predictions = self.model(inputs)\n",
    "        loss = self.loss_fn(predictions, target)\n",
    "        acc= (predictions.argmax(dim=1)==target).float().mean()\n",
    "        self.log(\"test_loss\", loss, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"test_acc\", acc, on_epoch=True, prog_bar=True)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
    "    \n",
    "    def on_train_epoch_end(self):\n",
    "        train_loss = self.trainer.logged_metrics.get(\"train_loss_epoch\")\n",
    "        print(f\"Epoch {self.current_epoch + 1}: train_loss = {train_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train the model using lightning\n",
    "# mlp = MLP(input_dim=28*28, output_dim=10)\n",
    "# lightning_model=MLPWrapper(mlp)\n",
    "# early_stoppping = L.pytorch.callbacks.EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=5, verbose=True)\n",
    "# trainer = L.Trainer(max_epochs=20, accelerator=\"auto\")\n",
    "# trainer.fit(lightning_model, train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for optuna optimization\n",
    "def objective(trial):\n",
    "    # define parameters to optimize\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 0.0001, 100)\n",
    "    dropout_prob = trial.suggest_float(\"dropout_prob\", 0.2, 0.5)\n",
    "\n",
    "    mlp = mlp = MLP(input_dim=28*28, output_dim=10, lr=0.0001)\n",
    "    lightning_model=MLPWrapper(mlp)\n",
    "    early_stoppping = L.pytorch.callbacks.EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=5, verbose=True)\n",
    "    trainer = L.Trainer(max_epochs=20, accelerator=\"auto\")\n",
    "    trainer.fit(lightning_model, train_dataloader, valid_dataloader)\n",
    "\n",
    "    model_path = f\"mlp_trials/mlp_trial_{trial.number}.pth\"\n",
    "    torch.save(mlp.state_dict(), model_path) # save model\n",
    "    trial.set_user_attr(\"model_path\", model_path) # set model path as user attribute\n",
    "    \n",
    "    return trainer.callback_metrics[\"val_acc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 09:48:43,399] A new study created in memory with name: no-name-5dbbc014-0639-442d-82cb-b4bc0b8ab2f7\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type             | Params | Mode \n",
      "-----------------------------------------------------\n",
      "0 | model   | MLP              | 134 K  | train\n",
      "1 | loss_fn | CrossEntropyLoss | 0      | train\n",
      "-----------------------------------------------------\n",
      "134 K     Trainable params\n",
      "0         Non-trainable params\n",
      "134 K     Total params\n",
      "0.539     Total estimated model params size (MB)\n",
      "25        Modules in train mode\n",
      "1         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\ml_win_old\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:475: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\User\\anaconda3\\envs\\ml_win_old\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:419: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\ml_win_old\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:419: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 938/938 [00:16<00:00, 57.34it/s, v_num=32, train_loss_step=0.356, train_acc_step=0.875, val_loss=0.309, val_acc=0.914, train_loss_epoch=0.766, train_acc_epoch=0.765]Epoch 1: train_loss = 0.7660\n",
      "Epoch 1: 100%|██████████| 938/938 [00:16<00:00, 56.98it/s, v_num=32, train_loss_step=0.223, train_acc_step=0.906, val_loss=0.185, val_acc=0.949, train_loss_epoch=0.290, train_acc_epoch=0.914] Epoch 2: train_loss = 0.2899\n",
      "Epoch 2: 100%|██████████| 938/938 [00:16<00:00, 56.29it/s, v_num=32, train_loss_step=0.233, train_acc_step=0.938, val_loss=0.132, val_acc=0.962, train_loss_epoch=0.204, train_acc_epoch=0.939] Epoch 3: train_loss = 0.2037\n",
      "Epoch 3: 100%|██████████| 938/938 [00:16<00:00, 56.88it/s, v_num=32, train_loss_step=0.0874, train_acc_step=0.938, val_loss=0.0993, val_acc=0.972, train_loss_epoch=0.157, train_acc_epoch=0.953]Epoch 4: train_loss = 0.1574\n",
      "Epoch 4: 100%|██████████| 938/938 [00:16<00:00, 56.80it/s, v_num=32, train_loss_step=0.423, train_acc_step=0.844, val_loss=0.0808, val_acc=0.976, train_loss_epoch=0.128, train_acc_epoch=0.961] Epoch 5: train_loss = 0.1279\n",
      "Epoch 5: 100%|██████████| 938/938 [00:16<00:00, 56.67it/s, v_num=32, train_loss_step=0.0458, train_acc_step=1.000, val_loss=0.062, val_acc=0.983, train_loss_epoch=0.109, train_acc_epoch=0.967] Epoch 6: train_loss = 0.1093\n",
      "Epoch 6: 100%|██████████| 938/938 [00:16<00:00, 55.39it/s, v_num=32, train_loss_step=0.307, train_acc_step=0.938, val_loss=0.0536, val_acc=0.985, train_loss_epoch=0.093, train_acc_epoch=0.972] Epoch 7: train_loss = 0.0930\n",
      "Epoch 7: 100%|██████████| 938/938 [00:16<00:00, 55.90it/s, v_num=32, train_loss_step=0.208, train_acc_step=0.938, val_loss=0.0419, val_acc=0.990, train_loss_epoch=0.0792, train_acc_epoch=0.976] Epoch 8: train_loss = 0.0792\n",
      "Epoch 8: 100%|██████████| 938/938 [00:16<00:00, 55.99it/s, v_num=32, train_loss_step=0.205, train_acc_step=0.906, val_loss=0.0351, val_acc=0.991, train_loss_epoch=0.071, train_acc_epoch=0.978]   Epoch 9: train_loss = 0.0710\n",
      "Epoch 9: 100%|██████████| 938/938 [00:16<00:00, 56.38it/s, v_num=32, train_loss_step=0.0284, train_acc_step=1.000, val_loss=0.0293, val_acc=0.993, train_loss_epoch=0.0629, train_acc_epoch=0.981]Epoch 10: train_loss = 0.0629\n",
      "Epoch 10: 100%|██████████| 938/938 [00:16<00:00, 57.07it/s, v_num=32, train_loss_step=0.0931, train_acc_step=0.969, val_loss=0.0227, val_acc=0.995, train_loss_epoch=0.0534, train_acc_epoch=0.983] Epoch 11: train_loss = 0.0534\n",
      "Epoch 11: 100%|██████████| 938/938 [00:16<00:00, 55.75it/s, v_num=32, train_loss_step=0.0249, train_acc_step=1.000, val_loss=0.0197, val_acc=0.996, train_loss_epoch=0.0461, train_acc_epoch=0.986] Epoch 12: train_loss = 0.0461\n",
      "Epoch 12: 100%|██████████| 938/938 [00:16<00:00, 56.25it/s, v_num=32, train_loss_step=0.0709, train_acc_step=0.969, val_loss=0.0165, val_acc=0.997, train_loss_epoch=0.0422, train_acc_epoch=0.986] Epoch 13: train_loss = 0.0422\n",
      "Epoch 13: 100%|██████████| 938/938 [00:16<00:00, 55.20it/s, v_num=32, train_loss_step=0.124, train_acc_step=0.969, val_loss=0.0128, val_acc=0.998, train_loss_epoch=0.0382, train_acc_epoch=0.988]  Epoch 14: train_loss = 0.0382\n",
      "Epoch 14: 100%|██████████| 938/938 [00:16<00:00, 56.41it/s, v_num=32, train_loss_step=0.00283, train_acc_step=1.000, val_loss=0.0126, val_acc=0.997, train_loss_epoch=0.0339, train_acc_epoch=0.989]Epoch 15: train_loss = 0.0339\n",
      "Epoch 15: 100%|██████████| 938/938 [00:17<00:00, 55.17it/s, v_num=32, train_loss_step=0.222, train_acc_step=0.906, val_loss=0.0112, val_acc=0.998, train_loss_epoch=0.0321, train_acc_epoch=0.990]  Epoch 16: train_loss = 0.0321\n",
      "Epoch 16: 100%|██████████| 938/938 [00:16<00:00, 56.23it/s, v_num=32, train_loss_step=0.0298, train_acc_step=0.969, val_loss=0.00828, val_acc=0.999, train_loss_epoch=0.0276, train_acc_epoch=0.992]Epoch 17: train_loss = 0.0276\n",
      "Epoch 17: 100%|██████████| 938/938 [00:17<00:00, 54.13it/s, v_num=32, train_loss_step=0.00899, train_acc_step=1.000, val_loss=0.00698, val_acc=0.999, train_loss_epoch=0.0246, train_acc_epoch=0.992] Epoch 18: train_loss = 0.0246\n",
      "Epoch 18: 100%|██████████| 938/938 [00:16<00:00, 56.56it/s, v_num=32, train_loss_step=0.0316, train_acc_step=0.969, val_loss=0.00585, val_acc=0.999, train_loss_epoch=0.0245, train_acc_epoch=0.992]  Epoch 19: train_loss = 0.0245\n",
      "Epoch 19: 100%|██████████| 938/938 [00:16<00:00, 56.09it/s, v_num=32, train_loss_step=0.0251, train_acc_step=1.000, val_loss=0.00472, val_acc=1.000, train_loss_epoch=0.0222, train_acc_epoch=0.993]  Epoch 20: train_loss = 0.0222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 938/938 [00:16<00:00, 55.96it/s, v_num=32, train_loss_step=0.0251, train_acc_step=1.000, val_loss=0.00472, val_acc=1.000, train_loss_epoch=0.0222, train_acc_epoch=0.993]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 09:54:22,546] Trial 0 finished with value: 0.9995833039283752 and parameters: {'weight_decay': 5.0, 'dropout_prob': 0.5}. Best is trial 0 with value: 0.9995833039283752.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type             | Params | Mode \n",
      "-----------------------------------------------------\n",
      "0 | model   | MLP              | 134 K  | train\n",
      "1 | loss_fn | CrossEntropyLoss | 0      | train\n",
      "-----------------------------------------------------\n",
      "134 K     Trainable params\n",
      "0         Non-trainable params\n",
      "134 K     Total params\n",
      "0.539     Total estimated model params size (MB)\n",
      "25        Modules in train mode\n",
      "1         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 938/938 [00:16<00:00, 56.55it/s, v_num=33, train_loss_step=0.431, train_acc_step=0.844, val_loss=0.320, val_acc=0.906, train_loss_epoch=0.799, train_acc_epoch=0.758]Epoch 1: train_loss = 0.7994\n",
      "Epoch 1: 100%|██████████| 938/938 [00:15<00:00, 58.86it/s, v_num=33, train_loss_step=0.397, train_acc_step=0.875, val_loss=0.191, val_acc=0.944, train_loss_epoch=0.292, train_acc_epoch=0.914] Epoch 2: train_loss = 0.2922\n",
      "Epoch 2: 100%|██████████| 938/938 [00:16<00:00, 57.16it/s, v_num=33, train_loss_step=0.412, train_acc_step=0.875, val_loss=0.137, val_acc=0.960, train_loss_epoch=0.207, train_acc_epoch=0.939] Epoch 3: train_loss = 0.2069\n",
      "Epoch 3: 100%|██████████| 938/938 [00:16<00:00, 57.79it/s, v_num=33, train_loss_step=0.198, train_acc_step=0.906, val_loss=0.107, val_acc=0.970, train_loss_epoch=0.162, train_acc_epoch=0.951] Epoch 4: train_loss = 0.1618\n",
      "Epoch 4: 100%|██████████| 938/938 [00:17<00:00, 54.52it/s, v_num=33, train_loss_step=0.240, train_acc_step=0.906, val_loss=0.0817, val_acc=0.976, train_loss_epoch=0.133, train_acc_epoch=0.959]Epoch 5: train_loss = 0.1328\n",
      "Epoch 5: 100%|██████████| 938/938 [00:16<00:00, 57.14it/s, v_num=33, train_loss_step=0.284, train_acc_step=0.938, val_loss=0.0662, val_acc=0.980, train_loss_epoch=0.111, train_acc_epoch=0.967] Epoch 6: train_loss = 0.1115\n",
      "Epoch 6: 100%|██████████| 938/938 [00:16<00:00, 57.38it/s, v_num=33, train_loss_step=0.0973, train_acc_step=0.969, val_loss=0.0517, val_acc=0.987, train_loss_epoch=0.0935, train_acc_epoch=0.972]Epoch 7: train_loss = 0.0935\n",
      "Epoch 7: 100%|██████████| 938/938 [00:16<00:00, 56.49it/s, v_num=33, train_loss_step=0.0746, train_acc_step=0.969, val_loss=0.0419, val_acc=0.989, train_loss_epoch=0.082, train_acc_epoch=0.975]  Epoch 8: train_loss = 0.0820\n",
      "Epoch 8: 100%|██████████| 938/938 [00:16<00:00, 57.00it/s, v_num=33, train_loss_step=0.263, train_acc_step=0.969, val_loss=0.034, val_acc=0.992, train_loss_epoch=0.0705, train_acc_epoch=0.978]  Epoch 9: train_loss = 0.0705\n",
      "Epoch 9: 100%|██████████| 938/938 [00:16<00:00, 56.45it/s, v_num=33, train_loss_step=0.0582, train_acc_step=0.969, val_loss=0.0286, val_acc=0.993, train_loss_epoch=0.0634, train_acc_epoch=0.980]Epoch 10: train_loss = 0.0634\n",
      "Epoch 10: 100%|██████████| 938/938 [00:16<00:00, 55.36it/s, v_num=33, train_loss_step=0.0443, train_acc_step=0.969, val_loss=0.024, val_acc=0.994, train_loss_epoch=0.056, train_acc_epoch=0.982]   Epoch 11: train_loss = 0.0560\n",
      "Epoch 11: 100%|██████████| 938/938 [00:16<00:00, 55.92it/s, v_num=33, train_loss_step=0.0533, train_acc_step=0.969, val_loss=0.0207, val_acc=0.996, train_loss_epoch=0.0484, train_acc_epoch=0.985]Epoch 12: train_loss = 0.0484\n",
      "Epoch 12: 100%|██████████| 938/938 [00:16<00:00, 56.41it/s, v_num=33, train_loss_step=0.158, train_acc_step=0.969, val_loss=0.0179, val_acc=0.996, train_loss_epoch=0.0442, train_acc_epoch=0.986]  Epoch 13: train_loss = 0.0442\n",
      "Epoch 13: 100%|██████████| 938/938 [00:16<00:00, 55.98it/s, v_num=33, train_loss_step=0.0577, train_acc_step=0.969, val_loss=0.0141, val_acc=0.997, train_loss_epoch=0.0386, train_acc_epoch=0.988] Epoch 14: train_loss = 0.0386\n",
      "Epoch 14: 100%|██████████| 938/938 [00:16<00:00, 56.09it/s, v_num=33, train_loss_step=0.0768, train_acc_step=0.938, val_loss=0.0129, val_acc=0.998, train_loss_epoch=0.0361, train_acc_epoch=0.989] Epoch 15: train_loss = 0.0361\n",
      "Epoch 15: 100%|██████████| 938/938 [00:17<00:00, 54.68it/s, v_num=33, train_loss_step=0.151, train_acc_step=0.969, val_loss=0.0101, val_acc=0.998, train_loss_epoch=0.0336, train_acc_epoch=0.989]  Epoch 16: train_loss = 0.0336\n",
      "Epoch 16: 100%|██████████| 938/938 [00:16<00:00, 55.75it/s, v_num=33, train_loss_step=0.00939, train_acc_step=1.000, val_loss=0.0104, val_acc=0.998, train_loss_epoch=0.0309, train_acc_epoch=0.990]Epoch 17: train_loss = 0.0309\n",
      "Epoch 17: 100%|██████████| 938/938 [00:16<00:00, 55.86it/s, v_num=33, train_loss_step=0.0827, train_acc_step=0.969, val_loss=0.00806, val_acc=0.999, train_loss_epoch=0.0269, train_acc_epoch=0.992]Epoch 18: train_loss = 0.0269\n",
      "Epoch 18: 100%|██████████| 938/938 [00:16<00:00, 56.09it/s, v_num=33, train_loss_step=0.103, train_acc_step=0.938, val_loss=0.00695, val_acc=0.999, train_loss_epoch=0.0248, train_acc_epoch=0.992]  Epoch 19: train_loss = 0.0248\n",
      "Epoch 19: 100%|██████████| 938/938 [00:16<00:00, 55.57it/s, v_num=33, train_loss_step=0.0186, train_acc_step=1.000, val_loss=0.00639, val_acc=0.999, train_loss_epoch=0.0236, train_acc_epoch=0.993]  Epoch 20: train_loss = 0.0236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 938/938 [00:16<00:00, 55.44it/s, v_num=33, train_loss_step=0.0186, train_acc_step=1.000, val_loss=0.00639, val_acc=0.999, train_loss_epoch=0.0236, train_acc_epoch=0.993]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 10:00:00,403] Trial 1 finished with value: 0.9991666674613953 and parameters: {'weight_decay': 5.0, 'dropout_prob': 0.2}. Best is trial 0 with value: 0.9995833039283752.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type             | Params | Mode \n",
      "-----------------------------------------------------\n",
      "0 | model   | MLP              | 134 K  | train\n",
      "1 | loss_fn | CrossEntropyLoss | 0      | train\n",
      "-----------------------------------------------------\n",
      "134 K     Trainable params\n",
      "0         Non-trainable params\n",
      "134 K     Total params\n",
      "0.539     Total estimated model params size (MB)\n",
      "25        Modules in train mode\n",
      "1         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 938/938 [00:16<00:00, 58.27it/s, v_num=34, train_loss_step=0.387, train_acc_step=0.875, val_loss=0.317, val_acc=0.911, train_loss_epoch=0.754, train_acc_epoch=0.767]Epoch 1: train_loss = 0.7542\n",
      "Epoch 1: 100%|██████████| 938/938 [00:16<00:00, 57.64it/s, v_num=34, train_loss_step=0.255, train_acc_step=0.875, val_loss=0.190, val_acc=0.948, train_loss_epoch=0.286, train_acc_epoch=0.916] Epoch 2: train_loss = 0.2863\n",
      "Epoch 2: 100%|██████████| 938/938 [00:16<00:00, 57.25it/s, v_num=34, train_loss_step=0.062, train_acc_step=1.000, val_loss=0.138, val_acc=0.961, train_loss_epoch=0.203, train_acc_epoch=0.939] Epoch 3: train_loss = 0.2027\n",
      "Epoch 3: 100%|██████████| 938/938 [00:16<00:00, 57.22it/s, v_num=34, train_loss_step=0.0831, train_acc_step=0.969, val_loss=0.107, val_acc=0.971, train_loss_epoch=0.161, train_acc_epoch=0.952]Epoch 4: train_loss = 0.1610\n",
      "Epoch 4: 100%|██████████| 938/938 [00:16<00:00, 57.36it/s, v_num=34, train_loss_step=0.115, train_acc_step=0.969, val_loss=0.0821, val_acc=0.978, train_loss_epoch=0.131, train_acc_epoch=0.961]Epoch 5: train_loss = 0.1311\n",
      "Epoch 5: 100%|██████████| 938/938 [00:16<00:00, 57.15it/s, v_num=34, train_loss_step=0.187, train_acc_step=0.938, val_loss=0.0674, val_acc=0.982, train_loss_epoch=0.110, train_acc_epoch=0.967] Epoch 6: train_loss = 0.1103\n",
      "Epoch 6: 100%|██████████| 938/938 [00:16<00:00, 57.83it/s, v_num=34, train_loss_step=0.0254, train_acc_step=1.000, val_loss=0.0556, val_acc=0.985, train_loss_epoch=0.0954, train_acc_epoch=0.971]Epoch 7: train_loss = 0.0954\n",
      "Epoch 7: 100%|██████████| 938/938 [00:16<00:00, 57.44it/s, v_num=34, train_loss_step=0.0451, train_acc_step=1.000, val_loss=0.0454, val_acc=0.988, train_loss_epoch=0.083, train_acc_epoch=0.974]  Epoch 8: train_loss = 0.0830\n",
      "Epoch 8: 100%|██████████| 938/938 [00:16<00:00, 57.92it/s, v_num=34, train_loss_step=0.251, train_acc_step=0.906, val_loss=0.0389, val_acc=0.988, train_loss_epoch=0.0702, train_acc_epoch=0.979] Epoch 9: train_loss = 0.0702\n",
      "Epoch 9: 100%|██████████| 938/938 [00:16<00:00, 57.72it/s, v_num=34, train_loss_step=0.0831, train_acc_step=0.969, val_loss=0.0323, val_acc=0.991, train_loss_epoch=0.0629, train_acc_epoch=0.980] Epoch 10: train_loss = 0.0629\n",
      "Epoch 10: 100%|██████████| 938/938 [00:16<00:00, 57.53it/s, v_num=34, train_loss_step=0.0146, train_acc_step=1.000, val_loss=0.0276, val_acc=0.993, train_loss_epoch=0.055, train_acc_epoch=0.983]  Epoch 11: train_loss = 0.0550\n",
      "Epoch 11: 100%|██████████| 938/938 [00:16<00:00, 57.00it/s, v_num=34, train_loss_step=0.237, train_acc_step=0.938, val_loss=0.0205, val_acc=0.995, train_loss_epoch=0.0471, train_acc_epoch=0.985] Epoch 12: train_loss = 0.0471\n",
      "Epoch 12: 100%|██████████| 938/938 [00:16<00:00, 57.49it/s, v_num=34, train_loss_step=0.00752, train_acc_step=1.000, val_loss=0.0193, val_acc=0.995, train_loss_epoch=0.0422, train_acc_epoch=0.987]Epoch 13: train_loss = 0.0422\n",
      "Epoch 13: 100%|██████████| 938/938 [00:16<00:00, 56.97it/s, v_num=34, train_loss_step=0.0505, train_acc_step=0.969, val_loss=0.0146, val_acc=0.997, train_loss_epoch=0.0393, train_acc_epoch=0.987] Epoch 14: train_loss = 0.0393\n",
      "Epoch 14: 100%|██████████| 938/938 [00:16<00:00, 56.62it/s, v_num=34, train_loss_step=0.0493, train_acc_step=0.969, val_loss=0.0146, val_acc=0.997, train_loss_epoch=0.0349, train_acc_epoch=0.989] Epoch 15: train_loss = 0.0349\n",
      "Epoch 15: 100%|██████████| 938/938 [00:16<00:00, 57.50it/s, v_num=34, train_loss_step=0.00475, train_acc_step=1.000, val_loss=0.012, val_acc=0.998, train_loss_epoch=0.0325, train_acc_epoch=0.990] Epoch 16: train_loss = 0.0325\n",
      "Epoch 16: 100%|██████████| 938/938 [00:16<00:00, 57.85it/s, v_num=34, train_loss_step=0.0614, train_acc_step=1.000, val_loss=0.00997, val_acc=0.998, train_loss_epoch=0.0273, train_acc_epoch=0.992]Epoch 17: train_loss = 0.0273\n",
      "Epoch 17: 100%|██████████| 938/938 [00:16<00:00, 56.78it/s, v_num=34, train_loss_step=0.00937, train_acc_step=1.000, val_loss=0.00707, val_acc=0.999, train_loss_epoch=0.0262, train_acc_epoch=0.992]Epoch 18: train_loss = 0.0262\n",
      "Epoch 18: 100%|██████████| 938/938 [00:16<00:00, 57.76it/s, v_num=34, train_loss_step=0.054, train_acc_step=0.969, val_loss=0.00707, val_acc=0.999, train_loss_epoch=0.0238, train_acc_epoch=0.993]   Epoch 19: train_loss = 0.0238\n",
      "Epoch 19: 100%|██████████| 938/938 [00:16<00:00, 57.62it/s, v_num=34, train_loss_step=0.0128, train_acc_step=1.000, val_loss=0.00608, val_acc=0.999, train_loss_epoch=0.022, train_acc_epoch=0.993]   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: train_loss = 0.0220\n",
      "Epoch 19: 100%|██████████| 938/938 [00:16<00:00, 57.49it/s, v_num=34, train_loss_step=0.0128, train_acc_step=1.000, val_loss=0.00608, val_acc=0.999, train_loss_epoch=0.022, train_acc_epoch=0.993]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 10:05:31,731] Trial 2 finished with value: 0.9992499947547913 and parameters: {'weight_decay': 2.0, 'dropout_prob': 0.2}. Best is trial 0 with value: 0.9995833039283752.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type             | Params | Mode \n",
      "-----------------------------------------------------\n",
      "0 | model   | MLP              | 134 K  | train\n",
      "1 | loss_fn | CrossEntropyLoss | 0      | train\n",
      "-----------------------------------------------------\n",
      "134 K     Trainable params\n",
      "0         Non-trainable params\n",
      "134 K     Total params\n",
      "0.539     Total estimated model params size (MB)\n",
      "25        Modules in train mode\n",
      "1         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 938/938 [00:16<00:00, 57.78it/s, v_num=35, train_loss_step=0.267, train_acc_step=0.938, val_loss=0.321, val_acc=0.909, train_loss_epoch=0.801, train_acc_epoch=0.754]Epoch 1: train_loss = 0.8009\n",
      "Epoch 1: 100%|██████████| 938/938 [00:16<00:00, 57.73it/s, v_num=35, train_loss_step=0.223, train_acc_step=0.938, val_loss=0.201, val_acc=0.942, train_loss_epoch=0.296, train_acc_epoch=0.913] Epoch 2: train_loss = 0.2959\n",
      "Epoch 2: 100%|██████████| 938/938 [00:15<00:00, 58.68it/s, v_num=35, train_loss_step=0.309, train_acc_step=0.906, val_loss=0.151, val_acc=0.957, train_loss_epoch=0.206, train_acc_epoch=0.938] Epoch 3: train_loss = 0.2064\n",
      "Epoch 3: 100%|██████████| 938/938 [00:16<00:00, 58.38it/s, v_num=35, train_loss_step=0.127, train_acc_step=0.938, val_loss=0.122, val_acc=0.966, train_loss_epoch=0.161, train_acc_epoch=0.951] Epoch 4: train_loss = 0.1612\n",
      "Epoch 4: 100%|██████████| 938/938 [00:16<00:00, 57.44it/s, v_num=35, train_loss_step=0.046, train_acc_step=1.000, val_loss=0.0922, val_acc=0.974, train_loss_epoch=0.133, train_acc_epoch=0.959]Epoch 5: train_loss = 0.1326\n",
      "Epoch 5: 100%|██████████| 938/938 [00:15<00:00, 58.71it/s, v_num=35, train_loss_step=0.410, train_acc_step=0.812, val_loss=0.070, val_acc=0.980, train_loss_epoch=0.114, train_acc_epoch=0.965]  Epoch 6: train_loss = 0.1142\n",
      "Epoch 6: 100%|██████████| 938/938 [00:16<00:00, 57.56it/s, v_num=35, train_loss_step=0.297, train_acc_step=0.906, val_loss=0.057, val_acc=0.983, train_loss_epoch=0.0952, train_acc_epoch=0.970] Epoch 7: train_loss = 0.0952\n",
      "Epoch 7: 100%|██████████| 938/938 [00:16<00:00, 56.50it/s, v_num=35, train_loss_step=0.0133, train_acc_step=1.000, val_loss=0.0506, val_acc=0.986, train_loss_epoch=0.0838, train_acc_epoch=0.975]Epoch 8: train_loss = 0.0838\n",
      "Epoch 8: 100%|██████████| 938/938 [00:16<00:00, 56.20it/s, v_num=35, train_loss_step=0.067, train_acc_step=0.969, val_loss=0.0464, val_acc=0.989, train_loss_epoch=0.0716, train_acc_epoch=0.978]  Epoch 9: train_loss = 0.0716\n",
      "Epoch 9: 100%|██████████| 938/938 [00:16<00:00, 57.30it/s, v_num=35, train_loss_step=0.0365, train_acc_step=1.000, val_loss=0.0319, val_acc=0.993, train_loss_epoch=0.0636, train_acc_epoch=0.980] Epoch 10: train_loss = 0.0636\n",
      "Epoch 10: 100%|██████████| 938/938 [00:16<00:00, 57.04it/s, v_num=35, train_loss_step=0.124, train_acc_step=0.969, val_loss=0.0272, val_acc=0.994, train_loss_epoch=0.0567, train_acc_epoch=0.982]  Epoch 11: train_loss = 0.0567\n",
      "Epoch 11: 100%|██████████| 938/938 [00:16<00:00, 57.46it/s, v_num=35, train_loss_step=0.0469, train_acc_step=1.000, val_loss=0.0241, val_acc=0.995, train_loss_epoch=0.0497, train_acc_epoch=0.984] Epoch 12: train_loss = 0.0497\n",
      "Epoch 12: 100%|██████████| 938/938 [00:16<00:00, 56.88it/s, v_num=35, train_loss_step=0.107, train_acc_step=0.969, val_loss=0.0183, val_acc=0.996, train_loss_epoch=0.044, train_acc_epoch=0.986]   Epoch 13: train_loss = 0.0440\n",
      "Epoch 13: 100%|██████████| 938/938 [00:16<00:00, 57.06it/s, v_num=35, train_loss_step=0.140, train_acc_step=0.938, val_loss=0.0248, val_acc=0.996, train_loss_epoch=0.0407, train_acc_epoch=0.987] Epoch 14: train_loss = 0.0407\n",
      "Epoch 14: 100%|██████████| 938/938 [00:16<00:00, 55.68it/s, v_num=35, train_loss_step=0.0142, train_acc_step=1.000, val_loss=0.0182, val_acc=0.997, train_loss_epoch=0.0368, train_acc_epoch=0.988] Epoch 15: train_loss = 0.0368\n",
      "Epoch 15: 100%|██████████| 938/938 [00:16<00:00, 57.10it/s, v_num=35, train_loss_step=0.0311, train_acc_step=1.000, val_loss=0.0125, val_acc=0.998, train_loss_epoch=0.0343, train_acc_epoch=0.990] Epoch 16: train_loss = 0.0343\n",
      "Epoch 16: 100%|██████████| 938/938 [00:16<00:00, 57.08it/s, v_num=35, train_loss_step=0.105, train_acc_step=0.938, val_loss=0.0114, val_acc=0.998, train_loss_epoch=0.0291, train_acc_epoch=0.991]  Epoch 17: train_loss = 0.0291\n",
      "Epoch 17: 100%|██████████| 938/938 [00:16<00:00, 57.69it/s, v_num=35, train_loss_step=0.061, train_acc_step=0.969, val_loss=0.0103, val_acc=0.999, train_loss_epoch=0.0273, train_acc_epoch=0.991]  Epoch 18: train_loss = 0.0273\n",
      "Epoch 18: 100%|██████████| 938/938 [00:17<00:00, 52.26it/s, v_num=35, train_loss_step=0.0553, train_acc_step=0.969, val_loss=0.00987, val_acc=0.999, train_loss_epoch=0.0253, train_acc_epoch=0.992] Epoch 19: train_loss = 0.0253\n",
      "Epoch 19: 100%|██████████| 938/938 [00:16<00:00, 55.72it/s, v_num=35, train_loss_step=0.00668, train_acc_step=1.000, val_loss=0.0109, val_acc=0.998, train_loss_epoch=0.0236, train_acc_epoch=0.992] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: train_loss = 0.0236\n",
      "Epoch 19: 100%|██████████| 938/938 [00:16<00:00, 55.62it/s, v_num=35, train_loss_step=0.00668, train_acc_step=1.000, val_loss=0.0109, val_acc=0.998, train_loss_epoch=0.0236, train_acc_epoch=0.992]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 10:11:05,726] Trial 3 finished with value: 0.9984166622161865 and parameters: {'weight_decay': 1.0, 'dropout_prob': 0.5}. Best is trial 0 with value: 0.9995833039283752.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type             | Params | Mode \n",
      "-----------------------------------------------------\n",
      "0 | model   | MLP              | 134 K  | train\n",
      "1 | loss_fn | CrossEntropyLoss | 0      | train\n",
      "-----------------------------------------------------\n",
      "134 K     Trainable params\n",
      "0         Non-trainable params\n",
      "134 K     Total params\n",
      "0.539     Total estimated model params size (MB)\n",
      "25        Modules in train mode\n",
      "1         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 938/938 [00:16<00:00, 57.64it/s, v_num=36, train_loss_step=0.178, train_acc_step=0.969, val_loss=0.304, val_acc=0.913, train_loss_epoch=0.734, train_acc_epoch=0.773]Epoch 1: train_loss = 0.7338\n",
      "Epoch 1: 100%|██████████| 938/938 [00:16<00:00, 58.03it/s, v_num=36, train_loss_step=0.405, train_acc_step=0.844, val_loss=0.183, val_acc=0.947, train_loss_epoch=0.285, train_acc_epoch=0.916] Epoch 2: train_loss = 0.2852\n",
      "Epoch 2: 100%|██████████| 938/938 [00:16<00:00, 58.47it/s, v_num=36, train_loss_step=0.058, train_acc_step=1.000, val_loss=0.132, val_acc=0.962, train_loss_epoch=0.200, train_acc_epoch=0.940] Epoch 3: train_loss = 0.2000\n",
      "Epoch 3: 100%|██████████| 938/938 [00:16<00:00, 56.05it/s, v_num=36, train_loss_step=0.112, train_acc_step=0.969, val_loss=0.102, val_acc=0.970, train_loss_epoch=0.158, train_acc_epoch=0.953] Epoch 4: train_loss = 0.1577\n",
      "Epoch 4: 100%|██████████| 938/938 [00:16<00:00, 57.16it/s, v_num=36, train_loss_step=0.241, train_acc_step=0.875, val_loss=0.0792, val_acc=0.976, train_loss_epoch=0.130, train_acc_epoch=0.961]Epoch 5: train_loss = 0.1298\n",
      "Epoch 5: 100%|██████████| 938/938 [00:16<00:00, 57.86it/s, v_num=36, train_loss_step=0.140, train_acc_step=0.969, val_loss=0.0624, val_acc=0.983, train_loss_epoch=0.107, train_acc_epoch=0.967] Epoch 6: train_loss = 0.1071\n",
      "Epoch 6: 100%|██████████| 938/938 [00:16<00:00, 57.00it/s, v_num=36, train_loss_step=0.039, train_acc_step=1.000, val_loss=0.0503, val_acc=0.987, train_loss_epoch=0.0929, train_acc_epoch=0.972] Epoch 7: train_loss = 0.0929\n",
      "Epoch 7: 100%|██████████| 938/938 [00:16<00:00, 56.43it/s, v_num=36, train_loss_step=0.164, train_acc_step=0.938, val_loss=0.0417, val_acc=0.990, train_loss_epoch=0.080, train_acc_epoch=0.976]   Epoch 8: train_loss = 0.0800\n",
      "Epoch 8: 100%|██████████| 938/938 [00:16<00:00, 56.94it/s, v_num=36, train_loss_step=0.102, train_acc_step=0.969, val_loss=0.0339, val_acc=0.992, train_loss_epoch=0.0694, train_acc_epoch=0.978] Epoch 9: train_loss = 0.0694\n",
      "Epoch 9: 100%|██████████| 938/938 [00:16<00:00, 56.75it/s, v_num=36, train_loss_step=0.0133, train_acc_step=1.000, val_loss=0.0277, val_acc=0.993, train_loss_epoch=0.060, train_acc_epoch=0.981]  Epoch 10: train_loss = 0.0600\n",
      "Epoch 10: 100%|██████████| 938/938 [00:16<00:00, 57.00it/s, v_num=36, train_loss_step=0.067, train_acc_step=0.969, val_loss=0.0227, val_acc=0.995, train_loss_epoch=0.053, train_acc_epoch=0.984]  Epoch 11: train_loss = 0.0530\n",
      "Epoch 11: 100%|██████████| 938/938 [00:16<00:00, 56.32it/s, v_num=36, train_loss_step=0.103, train_acc_step=0.969, val_loss=0.0186, val_acc=0.996, train_loss_epoch=0.046, train_acc_epoch=0.986]  Epoch 12: train_loss = 0.0460\n",
      "Epoch 12: 100%|██████████| 938/938 [00:16<00:00, 57.32it/s, v_num=36, train_loss_step=0.0131, train_acc_step=1.000, val_loss=0.016, val_acc=0.996, train_loss_epoch=0.0432, train_acc_epoch=0.987] Epoch 13: train_loss = 0.0432\n",
      "Epoch 13: 100%|██████████| 938/938 [00:16<00:00, 56.59it/s, v_num=36, train_loss_step=0.115, train_acc_step=0.938, val_loss=0.0132, val_acc=0.997, train_loss_epoch=0.0384, train_acc_epoch=0.988] Epoch 14: train_loss = 0.0384\n",
      "Epoch 14: 100%|██████████| 938/938 [00:16<00:00, 56.09it/s, v_num=36, train_loss_step=0.0689, train_acc_step=0.969, val_loss=0.0113, val_acc=0.998, train_loss_epoch=0.0348, train_acc_epoch=0.990] Epoch 15: train_loss = 0.0348\n",
      "Epoch 15: 100%|██████████| 938/938 [00:16<00:00, 55.20it/s, v_num=36, train_loss_step=0.0137, train_acc_step=1.000, val_loss=0.0094, val_acc=0.998, train_loss_epoch=0.0308, train_acc_epoch=0.990] Epoch 16: train_loss = 0.0308\n",
      "Epoch 16: 100%|██████████| 938/938 [00:16<00:00, 55.63it/s, v_num=36, train_loss_step=0.00183, train_acc_step=1.000, val_loss=0.00859, val_acc=0.999, train_loss_epoch=0.0288, train_acc_epoch=0.991]Epoch 17: train_loss = 0.0288\n",
      "Epoch 17: 100%|██████████| 938/938 [00:16<00:00, 55.80it/s, v_num=36, train_loss_step=0.110, train_acc_step=0.969, val_loss=0.00725, val_acc=0.999, train_loss_epoch=0.0261, train_acc_epoch=0.992]  Epoch 18: train_loss = 0.0261\n",
      "Epoch 18: 100%|██████████| 938/938 [00:16<00:00, 55.64it/s, v_num=36, train_loss_step=0.0845, train_acc_step=0.938, val_loss=0.00558, val_acc=0.999, train_loss_epoch=0.0237, train_acc_epoch=0.993] Epoch 19: train_loss = 0.0237\n",
      "Epoch 19: 100%|██████████| 938/938 [00:18<00:00, 50.50it/s, v_num=36, train_loss_step=0.0096, train_acc_step=1.000, val_loss=0.00554, val_acc=0.999, train_loss_epoch=0.0216, train_acc_epoch=0.994]  Epoch 20: train_loss = 0.0216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 938/938 [00:18<00:00, 50.40it/s, v_num=36, train_loss_step=0.0096, train_acc_step=1.000, val_loss=0.00554, val_acc=0.999, train_loss_epoch=0.0216, train_acc_epoch=0.994]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 10:16:43,272] Trial 4 finished with value: 0.9994166493415833 and parameters: {'weight_decay': 1.0, 'dropout_prob': 0.2}. Best is trial 0 with value: 0.9995833039283752.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type             | Params | Mode \n",
      "-----------------------------------------------------\n",
      "0 | model   | MLP              | 134 K  | train\n",
      "1 | loss_fn | CrossEntropyLoss | 0      | train\n",
      "-----------------------------------------------------\n",
      "134 K     Trainable params\n",
      "0         Non-trainable params\n",
      "134 K     Total params\n",
      "0.539     Total estimated model params size (MB)\n",
      "25        Modules in train mode\n",
      "1         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 938/938 [00:16<00:00, 57.27it/s, v_num=37, train_loss_step=0.481, train_acc_step=0.812, val_loss=0.316, val_acc=0.912, train_loss_epoch=0.753, train_acc_epoch=0.768]Epoch 1: train_loss = 0.7530\n",
      "Epoch 1: 100%|██████████| 938/938 [00:16<00:00, 56.73it/s, v_num=37, train_loss_step=0.180, train_acc_step=0.938, val_loss=0.185, val_acc=0.946, train_loss_epoch=0.283, train_acc_epoch=0.916] Epoch 2: train_loss = 0.2834\n",
      "Epoch 2: 100%|██████████| 938/938 [00:16<00:00, 55.75it/s, v_num=37, train_loss_step=0.135, train_acc_step=0.938, val_loss=0.132, val_acc=0.963, train_loss_epoch=0.199, train_acc_epoch=0.940] Epoch 3: train_loss = 0.1991\n",
      "Epoch 3: 100%|██████████| 938/938 [00:16<00:00, 55.62it/s, v_num=37, train_loss_step=0.253, train_acc_step=0.906, val_loss=0.101, val_acc=0.971, train_loss_epoch=0.157, train_acc_epoch=0.952] Epoch 4: train_loss = 0.1570\n",
      "Epoch 4: 100%|██████████| 938/938 [00:16<00:00, 56.46it/s, v_num=37, train_loss_step=0.180, train_acc_step=0.906, val_loss=0.0764, val_acc=0.980, train_loss_epoch=0.129, train_acc_epoch=0.962]Epoch 5: train_loss = 0.1288\n",
      "Epoch 5: 100%|██████████| 938/938 [00:16<00:00, 56.41it/s, v_num=37, train_loss_step=0.293, train_acc_step=0.844, val_loss=0.0631, val_acc=0.982, train_loss_epoch=0.107, train_acc_epoch=0.968] Epoch 6: train_loss = 0.1075\n",
      "Epoch 6: 100%|██████████| 938/938 [00:16<00:00, 56.33it/s, v_num=37, train_loss_step=0.038, train_acc_step=1.000, val_loss=0.0506, val_acc=0.987, train_loss_epoch=0.093, train_acc_epoch=0.971] Epoch 7: train_loss = 0.0930\n",
      "Epoch 7: 100%|██████████| 938/938 [00:16<00:00, 57.31it/s, v_num=37, train_loss_step=0.0282, train_acc_step=1.000, val_loss=0.0409, val_acc=0.990, train_loss_epoch=0.0792, train_acc_epoch=0.975]Epoch 8: train_loss = 0.0792\n",
      "Epoch 8: 100%|██████████| 938/938 [00:16<00:00, 57.60it/s, v_num=37, train_loss_step=0.0254, train_acc_step=1.000, val_loss=0.0344, val_acc=0.991, train_loss_epoch=0.0682, train_acc_epoch=0.980] Epoch 9: train_loss = 0.0682\n",
      "Epoch 9: 100%|██████████| 938/938 [00:16<00:00, 57.33it/s, v_num=37, train_loss_step=0.0761, train_acc_step=0.969, val_loss=0.0282, val_acc=0.992, train_loss_epoch=0.0605, train_acc_epoch=0.981] Epoch 10: train_loss = 0.0605\n",
      "Epoch 10: 100%|██████████| 938/938 [00:16<00:00, 56.43it/s, v_num=37, train_loss_step=0.197, train_acc_step=0.906, val_loss=0.0228, val_acc=0.994, train_loss_epoch=0.0547, train_acc_epoch=0.984]  Epoch 11: train_loss = 0.0547\n",
      "Epoch 11: 100%|██████████| 938/938 [00:16<00:00, 57.60it/s, v_num=37, train_loss_step=0.152, train_acc_step=0.938, val_loss=0.0194, val_acc=0.996, train_loss_epoch=0.0463, train_acc_epoch=0.985]  Epoch 12: train_loss = 0.0463\n",
      "Epoch 12: 100%|██████████| 938/938 [00:16<00:00, 57.49it/s, v_num=37, train_loss_step=0.0376, train_acc_step=0.969, val_loss=0.0165, val_acc=0.996, train_loss_epoch=0.0434, train_acc_epoch=0.986] Epoch 13: train_loss = 0.0434\n",
      "Epoch 13: 100%|██████████| 938/938 [00:16<00:00, 56.94it/s, v_num=37, train_loss_step=0.201, train_acc_step=0.969, val_loss=0.0132, val_acc=0.998, train_loss_epoch=0.0398, train_acc_epoch=0.988]  Epoch 14: train_loss = 0.0398\n",
      "Epoch 14: 100%|██████████| 938/938 [00:16<00:00, 56.50it/s, v_num=37, train_loss_step=0.0124, train_acc_step=1.000, val_loss=0.0112, val_acc=0.998, train_loss_epoch=0.032, train_acc_epoch=0.990]  Epoch 15: train_loss = 0.0320\n",
      "Epoch 15: 100%|██████████| 938/938 [00:16<00:00, 57.10it/s, v_num=37, train_loss_step=0.0888, train_acc_step=0.938, val_loss=0.0101, val_acc=0.998, train_loss_epoch=0.0296, train_acc_epoch=0.991]Epoch 16: train_loss = 0.0296\n",
      "Epoch 16: 100%|██████████| 938/938 [00:16<00:00, 57.02it/s, v_num=37, train_loss_step=0.00333, train_acc_step=1.000, val_loss=0.00969, val_acc=0.998, train_loss_epoch=0.0272, train_acc_epoch=0.992]Epoch 17: train_loss = 0.0272\n",
      "Epoch 17: 100%|██████████| 938/938 [00:16<00:00, 56.79it/s, v_num=37, train_loss_step=0.00501, train_acc_step=1.000, val_loss=0.00754, val_acc=0.999, train_loss_epoch=0.0271, train_acc_epoch=0.991]Epoch 18: train_loss = 0.0271\n",
      "Epoch 18: 100%|██████████| 938/938 [00:16<00:00, 56.43it/s, v_num=37, train_loss_step=0.0822, train_acc_step=0.969, val_loss=0.00702, val_acc=0.999, train_loss_epoch=0.0249, train_acc_epoch=0.992]  Epoch 19: train_loss = 0.0249\n",
      "Epoch 19: 100%|██████████| 938/938 [00:16<00:00, 56.08it/s, v_num=37, train_loss_step=0.104, train_acc_step=0.938, val_loss=0.00643, val_acc=0.999, train_loss_epoch=0.0224, train_acc_epoch=0.993]   Epoch 20: train_loss = 0.0224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 938/938 [00:16<00:00, 55.95it/s, v_num=37, train_loss_step=0.104, train_acc_step=0.938, val_loss=0.00643, val_acc=0.999, train_loss_epoch=0.0224, train_acc_epoch=0.993]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 10:22:18,987] Trial 5 finished with value: 0.9989166855812073 and parameters: {'weight_decay': 2.0, 'dropout_prob': 0.5}. Best is trial 0 with value: 0.9995833039283752.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial:\n",
      "  Value: 0.9995833039283752\n",
      "  Params: \n",
      "    weight_decay: 5.0\n",
      "    dropout_prob: 0.5\n"
     ]
    }
   ],
   "source": [
    "# Run Optuna optimization\n",
    "# use grid search\n",
    "search_space = { \"weight_decay\" : [1, 2, 5], \"dropout_prob\" : [0.2, 0.5] } \n",
    "sampler = optuna.samplers.GridSampler(search_space)\n",
    "study = optuna.create_study(direction='maximize', sampler=sampler)\n",
    "study.optimize(objective)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best trial:\")\n",
    "print(f\"  Value: {study.best_trial.value}\")\n",
    "print(\"  Params: \")\n",
    "for key, value in study.best_trial.params.items():\n",
    "    print(f\"    {key}: {value}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_win_old",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

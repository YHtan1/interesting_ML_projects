{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required libraries\n",
    "import torch\n",
    "from torch import nn\n",
    "#we are going to train on the MNIST dataset\n",
    "from torchvision.datasets import MNIST\n",
    "#use dataloader to avoid loading too much data at once and crashing the kernel due to insuffiient memory\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import transforms\n",
    "#for plotting out the data\n",
    "import matplotlib.pyplot as plt\n",
    "#progress bar\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download MNIST\n",
    "train=MNIST(root='./data',train=True,download=True,transform=transforms.ToTensor())\n",
    "\n",
    "test=MNIST(root='./data',train=True,download=True,transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the datasets\n",
    "train_load=DataLoader(train,batch_size=100,shuffle=True,num_workers=1)\n",
    "test_load=DataLoader(test,batch_size=100,shuffle=True,num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#view the data\n",
    "plt.figure(figsize=(10,10))\n",
    "#iterate through the dataset and print out images and its dimensions\n",
    "for i, (image,label) in enumerate(train_load):\n",
    "    if i<5:\n",
    "        print(image[0][0].shape)\n",
    "        plt.subplot(1,5,i+1)\n",
    "        plt.imshow(image[0].squeeze(),cmap='gray')\n",
    " \n",
    "    else:\n",
    "        break\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#100 stands for the number of images\n",
    "#1 channel- greyscale image\n",
    "#28x28 image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write MLP class\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        #inherit from nn.Module (base class for all torch modules)\n",
    "        super().__init__()\n",
    "\n",
    "        #build the model\n",
    "        self.layers=nn.Sequential(\n",
    "            #flatten first dimension-row 1, row 2......... \n",
    "            nn.Flatten(),\n",
    "            #1st layer-input: 28x28=784 dim, output: 500 dim in Dense linear layer\n",
    "            nn.Linear(28*28,500),\n",
    "            #apply Relu activation function\n",
    "            nn.ReLU(),\n",
    "            #2nd layer-input: 500 dim, output: 300 dim in Dense linear layer\n",
    "            nn.Linear(500,300),\n",
    "            #apply Relu activation function\n",
    "            nn.ReLU(),\n",
    "            #3rd layer-input: 300 dim, output: 200 dim in Dense linear layer\n",
    "            nn.Linear(300,200),\n",
    "            #apply BatchNorm to avoid blowing up the gradient (Layer Norm is for NLP/transformers-the normalization is done across the layer, while Batch Norm is on feature)\n",
    "            nn.BatchNorm1d(200),\n",
    "            #apply Relu activation function\n",
    "            nn.ReLU(),\n",
    "            #apply Dropout to prevent overfitting (done after activation layer)\n",
    "            nn.Dropout(0.1),\n",
    "\n",
    "            #we add in Batch Norm and Dropout in 3rd layer\n",
    "\n",
    "            #4th layer-input: 200 dim, output: 150 dim in Dense linear layer\n",
    "            nn.Linear(200,150),\n",
    "            #apply Relu activation function\n",
    "            nn.ReLU(),\n",
    "            #5th layer-input: 150 dim, output: 120 dim in Dense linear layer\n",
    "            nn.Linear(150,120),\n",
    "            #apply Relu activation function\n",
    "            nn.ReLU(),\n",
    "            #6th layer-input: 120 dim, output: 80 dim in Dense linear layer\n",
    "            nn.Linear(120,80),\n",
    "            #apply BatchNorm to avoid blowing up the gradient\n",
    "            nn.BatchNorm1d(80),\n",
    "            #apply Relu activation function\n",
    "            nn.ReLU(),\n",
    "            #apply Dropout to prevent overfitting\n",
    "            nn.Dropout(0.1),\n",
    "            #we add in Batch Norm and Dropout in 6th layer\n",
    "\n",
    "            #7th layer-input: 80 dim, output: 50 dim in Dense linear layer\n",
    "            nn.Linear(80,50),\n",
    "            #apply Relu activation function\n",
    "            nn.ReLU(),\n",
    "            #8th layer-input: 50 dim, output: 30 dim in Dense linear layer\n",
    "            nn.Linear(50,30),\n",
    "            #apply Relu activation function\n",
    "            nn.ReLU(),\n",
    "            #9th layer-input: 30 dim, output: 10 dim in Dense linear layer\n",
    "            #10 dim because we have 10 classes\n",
    "            nn.Linear(30,10)\n",
    "            \n",
    "        )\n",
    "    \n",
    "    def forward(self,x):\n",
    "        #forward pass-calculate the output\n",
    "        return self.layers(x)\n",
    "    \n",
    "    def fit(self,x,epochs=5):\n",
    "        #fit the model\n",
    "        #define loss function\n",
    "        loss_fn=nn.CrossEntropyLoss()\n",
    "        #define optimizer\n",
    "        opt=torch.optim.Adam(self.parameters(),lr=0.01)\n",
    "\n",
    "        #store current loss\n",
    "        curr_loss=0\n",
    "        #training loop\n",
    "        for i in range(epochs):\n",
    "            for j, data in enumerate(tqdm(train_load)):\n",
    "                #get inputs\n",
    "                inputs,targets=data\n",
    "\n",
    "                #zero the gradients before backward pass so that we do accumulate gradients from last epoch\n",
    "                opt.zero_grad()\n",
    "\n",
    "                #forward pass\n",
    "                y_pred=self.forward(inputs)\n",
    "\n",
    "                #calculate loss\n",
    "                loss=loss_fn(y_pred,targets)\n",
    "\n",
    "                #perform backward pass\n",
    "                loss.backward()\n",
    "\n",
    "                #update weights\n",
    "                opt.step()  \n",
    "\n",
    "                #update current loss\n",
    "                curr_loss+=loss.item()\n",
    "\n",
    "            print(f\"Epoch {i+1}: loss = {curr_loss}\")\n",
    "        print(\"Training Complete\")\n",
    "    \n",
    "    def predict(self,x):\n",
    "        #use model to generate predictions\n",
    "        y_pred=self.forward(x)\n",
    "        return y_pred\n",
    "\n",
    "    def evaluate_model(self,x,verbose=0):\n",
    "        #evaluate the model\n",
    "        self.eval()\n",
    "        #test loop\n",
    "        run_count_correct=0\n",
    "        total_count=0\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(tqdm(test_load)):\n",
    "                #get inputs\n",
    "                inputs,y_true=data\n",
    "\n",
    "                #generate predictions\n",
    "                y_pred=torch.argmax(self.predict(inputs),dim=1)\n",
    "                # print(y_pred)\n",
    "                # print(y_true)\n",
    "                #current batch accuracy\n",
    "                current_count=torch.sum(y_true==y_pred)\n",
    "                #print out the accuracy for each batch in loaded data if verbose is 1\n",
    "                if verbose==1:\n",
    "                    print(f\"Batch {i} Accuracy = \",current_count.item()/y_true.shape[0])\n",
    "                #add to correct count\n",
    "                run_count_correct+=current_count\n",
    "                total_count+=y_true.shape[0]\n",
    "        #calculate overall accuracy across all batches\n",
    "        overall_accuracy=run_count_correct/total_count\n",
    "        print(f\"Test Accuracy = {overall_accuracy}\")\n",
    "        return overall_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [00:14<00:00, 41.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss = 230.71447492018342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [00:14<00:00, 42.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: loss = 327.49846977740526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [00:14<00:00, 41.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: loss = 401.5778101347387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [00:13<00:00, 43.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: loss = 462.7518239431083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [00:13<00:00, 43.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: loss = 518.6849524877034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [00:13<00:00, 43.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: loss = 565.546609995421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [00:13<00:00, 43.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: loss = 606.9512591826497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [00:14<00:00, 42.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: loss = 643.9272723080358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [00:13<00:00, 43.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: loss = 678.802924661024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [00:13<00:00, 43.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: loss = 709.2817554392968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [00:14<00:00, 42.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: loss = 739.5804058825888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [00:14<00:00, 42.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: loss = 769.1124751819007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [00:14<00:00, 40.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: loss = 793.9047426044999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [00:15<00:00, 38.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: loss = 818.2968692544091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [00:14<00:00, 41.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: loss = 841.1840041727119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [00:14<00:00, 42.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: loss = 861.9759351472021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [00:13<00:00, 43.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: loss = 882.1352947563864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [00:14<00:00, 42.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: loss = 900.36204043834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [00:14<00:00, 42.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: loss = 920.5552262832643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [00:13<00:00, 43.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: loss = 939.7078334032412\n",
      "Training Complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [00:11<00:00, 54.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy = 0.9936166405677795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#fit the model\n",
    "mlp=MLP()\n",
    "mlp.fit(train_load,epochs=20)\n",
    "\n",
    "#evaluate the model on test set\n",
    "mlp_test_accuracy=mlp.evaluate_model(test_load)\n",
    "#after training for 10 epochs, the test accuracy is 0.98, indicating that the model performance is extremely good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write CNN class\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        #inherit from nn.Module (base class for all torch modules)\n",
    "        super().__init__()\n",
    "        \n",
    "        #build the model\n",
    "        self.layers=nn.Sequential(\n",
    "            #no need to flatten the image as we structure the CNN to take in 2d images\n",
    "            #1st layer- we take in 1 channel and output 5 channels, to capture different features of the image\n",
    "            #after convolution, we apply max pooling\n",
    "            #for example, edge detection, contrast of image......\n",
    "            nn.Conv2d(in_channels=1,out_channels=5,kernel_size=5,padding=2),\n",
    "            #calc output dim- [28+2*2-1*(5-1)-1]/1 +1 =28\n",
    "            #apply max pooling\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2),\n",
    "            #calc output dim- [28+2*0-1*(2-1)-1]/2 +1 = 14\n",
    "\n",
    "            #2nd layer - convolution - we take in 5 channels and output 10 channels,kernel still 5, padding 2\n",
    "            nn.Conv2d(in_channels=5,out_channels=10,kernel_size=5),\n",
    "            #calc output dim- [14+2*0-1*(5-1)-1]/1 +1 = 10\n",
    "            #apply max pooling\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2),\n",
    "            #calc output dim- [10+2*0-1*(2-1)-1]/2 +1 = 5\n",
    "\n",
    "            #flatten the output of the previous layer\n",
    "            nn.Flatten(),\n",
    "\n",
    "            #3rd layer- linear layer\n",
    "            #input dim- 7*7 from one channel, we have 10 channels- input dim: 7*7*10, output dim: 200\n",
    "            nn.Linear(5*5*10,200),\n",
    "            #this layer is important- without this layer, the gradient will blow up (accuracy is much lower)\n",
    "            #apply BatchNorm to avoid blowing up the gradient- input dim: 200\n",
    "            nn.BatchNorm1d(200),\n",
    "            #apply Relu activation function\n",
    "            nn.ReLU(),\n",
    "            #remove dropout since we have many samples, the model is likely to have \"seen\" all possible datapoints- able to to handle the test set\n",
    "            #apply Dropout to prevent overfitting (done after activation layer)\n",
    "            # nn.Dropout(0.1),\n",
    "\n",
    "            #4th layer- linear layer - input dim: 200, output dim: 50\n",
    "            nn.Linear(200,50),\n",
    "            #apply Relu activation function\n",
    "            nn.ReLU(),\n",
    "\n",
    "            #5th layer- linear layer - input dim: 50, output dim: 10 (as there are 10 classes)\n",
    "            nn.Linear(50,10),\n",
    "\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "    def fit(self,x,epochs=5):\n",
    "        #fit the model\n",
    "        #define loss function\n",
    "        loss_fn=nn.CrossEntropyLoss()\n",
    "        #define optimizer\n",
    "        opt=torch.optim.Adam(self.parameters(),lr=0.01)\n",
    "\n",
    "        #store current loss\n",
    "        curr_loss=0\n",
    "        #training loop\n",
    "        for i in range(epochs):\n",
    "            for j, data in enumerate(tqdm(train_load)):\n",
    "                #get inputs\n",
    "                inputs,targets=data\n",
    "\n",
    "                #zero the gradients before backward pass so that we do accumulate gradients from last epoch\n",
    "                opt.zero_grad()\n",
    "\n",
    "                #forward pass\n",
    "                y_pred=self.forward(inputs)\n",
    "\n",
    "                #calculate loss\n",
    "                loss=loss_fn(y_pred,targets)\n",
    "\n",
    "                #perform backward pass\n",
    "                loss.backward()\n",
    "\n",
    "                #update weights\n",
    "                opt.step()  \n",
    "\n",
    "                #update current loss\n",
    "                curr_loss+=loss.item()\n",
    "\n",
    "            print(f\"Epoch {i+1}: loss = {curr_loss}\")\n",
    "        print(\"Training Complete\")\n",
    "    \n",
    "    def predict(self,x):\n",
    "        y_pred=self.forward(x)\n",
    "        return y_pred\n",
    "\n",
    "    def evaluate_model(self,x,verbose=0):\n",
    "        #evaluate the model\n",
    "        self.eval()\n",
    "        #test loop\n",
    "        run_count_correct=0\n",
    "        total_count=0\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(tqdm(test_load)):\n",
    "                #get inputs\n",
    "                inputs,y_true=data\n",
    "\n",
    "                #generate predictions\n",
    "                y_pred=torch.argmax(self.predict(inputs),dim=1)\n",
    "                # print(y_pred)\n",
    "                # print(y_true)\n",
    "                #current batch accuracy\n",
    "                current_count=torch.sum(y_true==y_pred)\n",
    "                #print out the accuracy for each batch in loaded data if verbose is 1\n",
    "                if verbose==1:\n",
    "                    print(f\"Batch {i} Accuracy = \",current_count.item()/y_true.shape[0])\n",
    "                #add to correct count\n",
    "                run_count_correct+=current_count\n",
    "                total_count+=y_true.shape[0]\n",
    "        #calculate overall accuracy across all batches\n",
    "        overall_accuracy=run_count_correct/total_count\n",
    "        print(f\"Test Accuracy = {overall_accuracy}\")\n",
    "        return overall_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [00:12<00:00, 48.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss = 84.33466117735952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [00:12<00:00, 49.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: loss = 120.99040624883492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [00:12<00:00, 49.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: loss = 148.81841257901397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [00:12<00:00, 47.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: loss = 173.6414594001253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [00:12<00:00, 46.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: loss = 193.2544918135536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [00:12<00:00, 47.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: loss = 212.26148284324154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [00:12<00:00, 47.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: loss = 232.1623952161317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [00:13<00:00, 45.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: loss = 248.3615630832428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [00:13<00:00, 46.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: loss = 264.4115319979828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [00:12<00:00, 47.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: loss = 279.2890466589088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [00:12<00:00, 47.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: loss = 292.9352425029647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [00:12<00:00, 48.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: loss = 308.16105712741046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [00:12<00:00, 47.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: loss = 321.04628788137416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [00:12<00:00, 48.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: loss = 335.58414193182034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [00:12<00:00, 48.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: loss = 346.2628908612751\n",
      "Training Complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [00:12<00:00, 49.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy = 0.9965166449546814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cnn=CNN()\n",
    "cnn.fit(train_load,epochs=15)\n",
    "\n",
    "#evaluate the model on test set\n",
    "cnn_test_accuracy=cnn.evaluate_model(test_load)\n",
    "#after training for 10 epochs, the test accuracy is 0.996, indicating that the model performance is extremely good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we convert pytorch tensor to a numpy array to read into decision tree\n",
    "#reshape train data into flat array after converting into np array\n",
    "train_data=train.data.numpy()\n",
    "train_data=train_data.reshape(60000,28*28)\n",
    "#reshape test data into flat array after converting into np array\n",
    "test_data=test.data.numpy()\n",
    "test_data=test_data.reshape(60000,28*28)\n",
    "#convert target into np array\n",
    "train_target=train.targets.numpy()\n",
    "test_target=test.targets.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy =  0.9950166666666667\n"
     ]
    }
   ],
   "source": [
    "#other than the neural network, we also use decision tree models\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#we use grid search to choose best parameters since the number of parameters available for optimization is quite little\n",
    "#if not Bayes Opt using Optuna or Hyperopt can work too \n",
    "\n",
    "#create decision tree object\n",
    "dec_tree=DecisionTreeClassifier(max_depth=20)\n",
    "\n",
    "#perform CV to choose best parameters\n",
    "#min_weight_fraction_leaf probably not needed- since so many samples to train on-even 0.01 is 600 samples in that leaf\n",
    "#since there are so many samples, we may always have enough samples to perform a split\n",
    "#probably we can use pruning to improve model generalization ability\n",
    "#increasing depth increases accuracy, however CV loss does not improve much even after increasing depth- we fix it at 20\n",
    "#tested increasing pruning parameter to increase generalization power, however we have many samples to train on- the model has \"seen\" all types of datapoints\n",
    "\n",
    "# param={\"ccp_alpha\":np.arange(0,1,0.5,dtype=\"float\")}\n",
    "# dec_treeCV=GridSearchCV(dec_tree,param)\n",
    "# dec_treeCV.fit(train_data,train_target)\n",
    "# #extract best model\n",
    "# dec_tree_best=dec_treeCV.best_estimator_\n",
    "\n",
    "#fit decision tree on train data\n",
    "dec_tree.fit(train_data,train_target)\n",
    "\n",
    "#generate predictions\n",
    "dec_tree_y_pred=dec_tree.predict(test_data)\n",
    "dec_tree_accuracy=np.mean((dec_tree_y_pred==test_target))\n",
    "print(\"Test Accuracy = \",dec_tree_accuracy)\n",
    "#trees can perform well even when compared to neural network models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy =  0.9992333333333333\n"
     ]
    }
   ],
   "source": [
    "#try random forest to see if it performs better than decision trees \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#create random forest object and fit random forest\n",
    "rand_for=RandomForestClassifier(max_depth=20)\n",
    "rand_for.fit(train_data,train_target)\n",
    "\n",
    "#generate predictions\n",
    "rand_for_y_pred=rand_for.predict(test_data)\n",
    "rand_for_accuracy=np.mean((rand_for_y_pred==test_target))\n",
    "print(\"Test Accuracy = \",rand_for_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy =  0.9981666666666666\n"
     ]
    }
   ],
   "source": [
    "#try xgboost to see if it performs better than decision trees and random forests\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "#create xgboost object and fit xgboost\n",
    "#deeper tree vs shallower tree - deeper tree use all samples to produce splits, shallower tree uses wrongly classified samples to further train\n",
    "#we can probably use colsample to random choose columns used in tree growing\n",
    "\n",
    "#create xgboost object\n",
    "#set n_estimators=50- grow shallow trees to reduce training time (deep trees are too resource hungry)\n",
    "#tree depth*number of trees=total depth will be much deeper than just unboosted trees to reach similar accuracy (training on wrong instances )\n",
    "#subsample- likely to reduce accuracy (use when we suspect that we are overfitting to improve model generalization)\n",
    "xgboost=XGBClassifier(device=\"cuda\",n_estimators=40)\n",
    "#sequential calculation- using cuda did not help much\n",
    "xgboost.fit(train_data,train_target)\n",
    "\n",
    "#generate predictions\n",
    "xgboost_y_pred=xgboost.predict(test_data)\n",
    "xgboost_accuracy=np.mean((xgboost_y_pred==test_target))\n",
    "print(\"Test Accuracy = \",xgboost_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy =  0.9973666666666666\n"
     ]
    }
   ],
   "source": [
    "#now we try xgboost random forest\n",
    "#create xgboost object and set num_parallel_tree=10, using 10 trees\n",
    "#the sqrt setting 28/784=0.035 is too low-the tree is less likely the best available split (small datasets- sqrt(20)/20 ~0.25 so still likely to choose the best feature for splitting)\n",
    "#we can also use subsampling to sample data for each boosting iteration\n",
    "rf_xgboost=XGBClassifier(device=\"cuda\",n_estimators=40,colsample_bynode=0.25,num_parallel_tree=10)\n",
    "\n",
    "#fit xgboost random forest\n",
    "rf_xgboost.fit(train_data,train_target)\n",
    "\n",
    "#generate predictions\n",
    "rf_xgboost_y_pred=rf_xgboost.predict(test_data)\n",
    "rf_xgboost_accuracy=np.mean((rf_xgboost_y_pred==test_target))\n",
    "print(\"Test Accuracy = \",rf_xgboost_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒═══════════════════════╤═════════════════╕\n",
      "│ Model                 │ Test Accuracy   │\n",
      "╞═══════════════════════╪═════════════════╡\n",
      "│ MLP                   │ 99.4%           │\n",
      "├───────────────────────┼─────────────────┤\n",
      "│ Decision Tree         │ 99.5%           │\n",
      "├───────────────────────┼─────────────────┤\n",
      "│ Random Forest         │ 99.9%           │\n",
      "├───────────────────────┼─────────────────┤\n",
      "│ CNN                   │ 99.7%           │\n",
      "├───────────────────────┼─────────────────┤\n",
      "│ XGBoost               │ 99.8%           │\n",
      "├───────────────────────┼─────────────────┤\n",
      "│ XGBoost RF (10 trees) │ 99.7%           │\n",
      "╘═══════════════════════╧═════════════════╛\n"
     ]
    }
   ],
   "source": [
    "#table to show the accuracy of different models\n",
    "#for MLP, the accuracy is a tensor- need to use .item() to extract the value\n",
    "results=pd.DataFrame({\"Model\":[\"MLP\",\"Decision Tree\",\"Random Forest\",\"CNN\",\"XGBoost\",\"XGBoost RF (10 trees)\"],\n",
    "               \"Test Accuracy\":[mlp_test_accuracy.item(),dec_tree_accuracy,rand_for_accuracy,cnn_test_accuracy.item(),xgboost_accuracy,rf_xgboost_accuracy]})\n",
    "results[\"Test Accuracy\"]=(results[\"Test Accuracy\"].round(decimals=3)*100).astype(str).add(\"%\")\n",
    "\n",
    "from tabulate import tabulate\n",
    "results_list=results.values.tolist()\n",
    "print(tabulate(results_list,headers=[\"Model\",\"Test Accuracy\"],tablefmt=\"fancy_grid\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_win_old",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
